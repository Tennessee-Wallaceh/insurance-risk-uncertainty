---
title: "Regression for Scaled Data"
author: "Yuqi Zhang"
date: "2024-06-10"
output: pdf_document
---

## Data Preparation for the Three Datasets

```{r}
scaled_data_train <- cbind(info_tr_train$ClaimNb_cap/info_tr_train$Exposure,info_tr_train)
colnames(scaled_data_train) <- c("Scaled_ClaimNb",colnames(info_tr_train))
scaled_data_train <- scaled_data_train[, -c(2,3,13)]

scaled_data_test <- cbind(info_tr_test$ClaimNb_cap/info_tr_test$Exposure,info_tr_test)
colnames(scaled_data_test) <- c("Scaled_ClaimNb",colnames(info_tr_test))
scaled_data_test <- scaled_data_test[, -c(2,3,13)]

scaled_data_cal <- cbind(info_tr_cal$ClaimNb_cap/info_tr_cal$Exposure,info_tr_cal)
colnames(scaled_data_cal) <- c("Scaled_ClaimNb",colnames(info_tr_cal))
scaled_data_cal <- scaled_data_cal[, -c(2,3,13)]
```

```{r}
# Train
# log-transformation
scaled_data_train$log_Scaled_ClaimNb <- log(scaled_data_train$Scaled_ClaimNb + 1)
#scaled_data_train <- #scaled_data_train[scaled_data_train$Scaled_ClaimNb < quantile(scaled_data_train$Scaled_ClaimNb, 0.99),]

# Categorical to factors
scaled_data_train$Area <- as.factor(scaled_data_train$Area)
scaled_data_train$VehBrand <- as.factor(scaled_data_train$VehBrand)
scaled_data_train$VehGas <- as.factor(scaled_data_train$VehGas)
scaled_data_train$Region <- as.factor(scaled_data_train$Region)

scaled_data_train <- scaled_data_train[, -c(1)]

# Test
# log-transformation
scaled_data_test$log_Scaled_ClaimNb <- log(scaled_data_test$Scaled_ClaimNb + 1)
#scaled_data_test <- #scaled_data_test[scaled_data_test$Scaled_ClaimNb < quantile(scaled_data_test$Scaled_ClaimNb, 0.99),]

# Categorical to factors
scaled_data_test$Area <- as.factor(scaled_data_test$Area)
scaled_data_test$VehBrand <- as.factor(scaled_data_test$VehBrand)
scaled_data_test$VehGas <- as.factor(scaled_data_test$VehGas)
scaled_data_test$Region <- as.factor(scaled_data_test$Region)

scaled_data_test <- scaled_data_test[, -c(1)]

# Cal
# log-transformation
scaled_data_cal$log_Scaled_ClaimNb <- log(scaled_data_cal$Scaled_ClaimNb + 1)

# Categorical to factors
scaled_data_cal$Area <- as.factor(scaled_data_cal$Area)
scaled_data_cal$VehBrand <- as.factor(scaled_data_cal$VehBrand)
scaled_data_cal$VehGas <- as.factor(scaled_data_cal$VehGas)
scaled_data_cal$Region <- as.factor(scaled_data_cal$Region)

scaled_data_cal <- scaled_data_cal[, -c(1)]
```

## GLM

```{r message=FALSE}
library(nnet)
library(MASS)

# GLM
formula <- log_Scaled_ClaimNb ~as.factor(Area) + VehPower + VehAge + DrivAge + BonusMalus + as.factor(VehBrand) + as.factor(VehGas) + as.factor(Region) + Density

glm_model <- glm(formula, data = scaled_data_train, family = gaussian())
```
## XGBoost

```{r message=FALSE}
library(caret)
library(xgboost)

X_train <- scaled_data_train[, -c(10)]
y_train <- scaled_data_train$log_Scaled_ClaimNb
X_test <- scaled_data_test[, -c(10)]
y_test <- scaled_data_test$log_Scaled_ClaimNb
X_cal <- scaled_data_cal[, -c(10)]
y_cal <- scaled_data_cal$log_Scaled_ClaimNb

categorical_features <- sapply(X_train, is.factor)
X_train[categorical_features] <- lapply(X_train[categorical_features], function(x) as.integer(as.factor(x)))

categorical_features <- sapply(X_test, is.factor)
X_test[categorical_features] <- lapply(X_test[categorical_features], function(x) as.integer(as.factor(x)))

categorical_features <- sapply(X_cal, is.factor)
X_cal[categorical_features] <- lapply(X_cal[categorical_features], function(x) as.integer(as.factor(x)))

X_train <- as.matrix(X_train)
X_test <- as.matrix(X_test)
X_cal <- as.matrix(X_cal)

# Converting to DMatrix for XGBoost
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest <- xgb.DMatrix(data = X_test, label = y_test)
dcal <- xgb.DMatrix(data = X_cal, label = y_cal)
```

```{r}
# XGBoost model parameters
params <- list(
  booster = "gbtree",
  objective = "reg:squaredlogerror",
  eta = 0.1,
  max_depth = 6,
  subsample = 0.5,
  colsample_bytree = 0.7
)

# XGBoost model
xgb_model <- xgboost(params = params, data = dtrain, nrounds = 100, verbose = 0)

```


## Performance Evaluation

```{r}
calculate_gini <- function(actual, predicted) {

  df <- data.frame(actual = actual, predicted = predicted)
  df <- df[order(df$predicted),]

  # Calculate the cumulative sums of actual values
  cum_actuals <- cumsum(df$actual) / sum(df$actual)
  cum_predicted = cumsum(df$predicted) / sum(df$predicted)
  
  # Area under the Lorenz curve
  Lorenz = cumsum(sort(df$actual) / sum(df$actual))
  B = sum(Lorenz[-length(Lorenz)]) / (length(Lorenz) - 1)

  # Area above the Lorenz curve
  A = 0.5 - B
  gini = (A / 0.5)

  return(gini)
}

```

### For predictor ClaimNb/Exposure

```{r}
# GLM
predictions_glm <- exp(predict(glm_model, newdata = scaled_data_test, type = "response")) - 1

hist(predictions_glm)

# MSE for GLM
mse_glm <- mean((predictions_glm - info_tr_test$ClaimNb_cap/info_tr_test$Exposure)^2)
print(paste("GLM MSE:", mse_glm))

# Gini for GLM
gini_glm <- calculate_gini(info_tr_test$ClaimNb_cap/info_tr_test$Exposure, predictions_glm)
print(paste("GLM Gini:", gini_glm))

```

```{r}
# XGBoost
predictions_xgb <- predict(xgb_model, dtest)
predictions_xgb <- exp(predictions_xgb) - 1
hist(predictions_xgb)

# MSE for XGBoost
mse_xgb <- mean((predictions_xgb - info_tr_test$ClaimNb_cap/info_tr_test$Exposure)^2)
print(paste("XGBoost MSE:", mse_xgb))

# Gini for XGBoost
gini_xgb <- calculate_gini(info_tr_test$ClaimNb_cap/info_tr_test$Exposure, predictions_xgb)
print(paste("XGBoost Gini:", gini_xgb))

```
The very high Gini coefficients and show that GLM and XGBoost both effectively discriminate between different outcomes. It ranks the predictions very well relative to the actual data.


### For non-zero claims

```{r}
nonzero_indices <- which(info_tr_test$ClaimNb_cap/info_tr_test$Exposure > 0)

mse_nonzero_glm <- mean((predictions_glm[nonzero_indices] - (info_tr_test$ClaimNb_cap/info_tr_test$Exposure)[nonzero_indices])^2)

mse_nonzero_xgb <- mean((predictions_xgb[nonzero_indices] - (info_tr_test$ClaimNb_cap/info_tr_test$Exposure)[nonzero_indices])^2)

print(paste("GLM MSE for Nonzero Claims:", mse_nonzero_glm))
print(paste("XGBoost MSE for Nonzero Claims:", mse_nonzero_xgb))

```

### Transform to binary 

```{r}
hist(predictions_glm)
hist(predictions_xgb)

predicted_claimNb_glm <- predictions_glm * info_tr_test$Exposure
predicted_claimNb_xgb <- predictions_xgb * info_tr_test$Exposure

hist(predicted_claimNb_glm)
hist(predicted_claimNb_xgb)
```

```{r}
# Binary classification of predictions
binary_predictions_glm <- ifelse(predicted_claimNb_glm > 0.05, 1, 0)
binary_predictions_xgb <- ifelse(predicted_claimNb_xgb > 0.05, 1, 0)

# Actual binary values
binary_actuals <- ifelse(info_test$ClaimNb_cap > 0, 1, 0)

```

```{r}
length(binary_predictions_glm)
length(binary_predictions_xgb)
length(binary_actuals)
```

```{r}
library(caret)

# Confusion matrix for GLM
confusion_matrix_glm <- confusionMatrix(factor(binary_predictions_glm), factor(binary_actuals))
print(confusion_matrix_glm)

# Confusion matrix for XGBoost
confusion_matrix_xgb <- confusionMatrix(factor(binary_predictions_xgb), factor(binary_actuals))
print(confusion_matrix_xgb)

mean((binary_predictions_glm-binary_actuals)^2)
mean((binary_predictions_xgb-binary_actuals)^2)

calculate_gini(predicted_claimNb_glm, info_tr_test$ClaimNb_cap)
calculate_gini(predicted_claimNb_xgb, info_tr_test$ClaimNb_cap)
```


```{r message=FALSE}
min_length <- min(length(binary_predictions_glm), length(binary_predictions_xgb), length(binary_actuals))

data_for_plot <- data.frame(
  Actual = binary_actuals[1:min_length],
  GLM_Predictions = binary_predictions_glm[1:min_length],
  XGB_Predictions = binary_predictions_xgb[1:min_length]
)

library(reshape2)

melted_data <- melt(data_for_plot, id.vars = NULL, variable.name = "Model", value.name = "Claim Status")

```
```{r}
library(ggplot2)

ggplot(melted_data, aes(x = as.factor(`Claim Status`), fill = Model)) +
  geom_bar(position = "dodge", stat="count") +
  scale_x_discrete(labels = c("Zero", "Non-Zero")) + 
  labs(title = "Comparison of Actual and Predicted Claims (Binary)",
       x = "Claim Status",
       y = "Count",
       fill = "Model Type") +
  facet_wrap(~ Model, scales = "free_y") + 
  theme_minimal()


```

## Output Predicted Probabilities

### For each observation vector

Typically, GLM and XGBoost do not output probabilities directly for regression tasks. Instead, they provide direct predictions of the outcome variable. We provide two attempts here.

**Attempt 1.** Normalise the predictions so they range between 0 and 1, which can be interpreted as probabilities.

```{r}
# Normalise function
normalise <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

# Calculate predictions for GLM and XGBoost
predictions_glm_test <- exp(predict(glm_model, newdata = scaled_data_test, type = "response")) - 1
predictions_xgb_test <- exp(predict(xgb_model, dtest)) - 1
predictions_glm_cal <- exp(predict(glm_model, newdata = scaled_data_cal, type = "response")) - 1
predictions_xgb_cal <- exp(predict(xgb_model, dcal)) - 1

# Normalise predictions to get 'probabilities'
Norm_probabilities_glm_test <- normalise(predictions_glm_test)
Norm_probabilities_xgb_test <- normalise(predictions_xgb_test)
Norm_probabilities_glm_cal <- normalise(predictions_glm_cal)
Norm_probabilities_xgb_cal <- normalise(predictions_xgb_cal)

# Output probabilities
print(Norm_probabilities_glm_test)
print(Norm_probabilities_xgb_test)

hist(Norm_probabilities_glm_test)
hist(Norm_probabilities_xgb_test)

print(Norm_probabilities_glm_cal)
print(Norm_probabilities_xgb_cal)

hist(Norm_probabilities_glm_cal)
hist(Norm_probabilities_xgb_cal)


```
**Attempt 2.** Logistic Transformation - map any real-valued number into the (0, 1) interval, treating the output as a log-odds and converting it to a probability. (With logistic function $1/(1+e^{-x})$)

```{r}
# Logistic transformation
logistic <- function(x) {
  1 / (1 + exp(-x))
}

# Apply logistic transformation
log_probabilities_glm_test <- logistic(predictions_glm_test)
log_probabilities_xgb_test <- logistic(predictions_xgb_test)
log_probabilities_glm_cal <- logistic(predictions_glm_cal)
log_probabilities_xgb_cal <- logistic(predictions_xgb_cal)

# Output probabilities
print(log_probabilities_glm_test)
print(log_probabilities_xgb_test)

hist(log_probabilities_glm_test)
hist(log_probabilities_xgb_test)

print(log_probabilities_glm_cal)
print(log_probabilities_xgb_cal)

hist(log_probabilities_glm_cal)
hist(log_probabilities_xgb_cal)
```


### For binary classifier

```{r}
# Converting the regression model output to probabilities for classification
predictions_glm_test_non_neg <- pmax(predictions_glm_test, 0)
predictions_xgb_test_non_neg <- pmax(predictions_xgb_test, 0)
predictions_glm_cal_non_neg <- pmax(predictions_glm_cal, 0)
predictions_xgb_cal_non_neg <- pmax(predictions_xgb_cal, 0)

# Convert these to probabilities assuming a logistic model
bin_probabilities_glm_test <- 1 / (1 + exp(-predictions_glm_test_non_neg))
bib_probabilities_xgb_test <- 1 / (1 + exp(-predictions_xgb_test_non_neg))
bin_probabilities_glm_cal <- 1 / (1 + exp(-predictions_glm_cal_non_neg))
bib_probabilities_xgb_cal <- 1 / (1 + exp(-predictions_xgb_cal_non_neg))

# Binary predictions using a threshold
binary_predictions_glm_prob_test <- ifelse(bin_probabilities_glm_test > 0.51, 1, 0)
binary_predictions_xgb_prob_test <- ifelse(bib_probabilities_xgb_test > 0.52, 1, 0)
binary_predictions_glm_prob_cal <- ifelse(bin_probabilities_glm_cal > 0.51, 1, 0)
binary_predictions_xgb_prob_cal <- ifelse(bib_probabilities_xgb_cal > 0.52, 1, 0)

# output the probabilities and binary predictions
print(bin_probabilities_glm_test)
print(bib_probabilities_xgb_test)
print(binary_predictions_glm_prob_test)
print(binary_predictions_xgb_prob_test)

print(bin_probabilities_glm_cal)
print(bib_probabilities_xgb_cal)
print(binary_predictions_glm_prob_cal)
print(binary_predictions_xgb_prob_cal)

hist(bin_probabilities_glm_test)
hist(bib_probabilities_xgb_test)
hist(bin_probabilities_glm_cal)
hist(bib_probabilities_xgb_cal)

```


## Output probabilities for ClaimNb individually


```{r}
library(stats)

# Calculate probabilities
calc_lognormal_probs <- function(predictions, exposure) {
  probs <- sapply(0:4, function(k) {
    lower_bound <- k / exposure
    upper_bound <- (k + 1) / exposure
    p_lower <- plnorm(lower_bound, meanlog = mean(predictions), sdlog = sd(predictions))
    p_upper <- plnorm(upper_bound, meanlog = mean(predictions), sdlog = sd(predictions))
    p_upper - p_lower
  })
  return(probs)
}

# Calculate probabilities for GLM and XGBoost models
prob_glm <- calc_lognormal_probs(predictions_glm, mean(info_tr_test$Exposure))
prob_xgb <- calc_lognormal_probs(predictions_xgb, mean(info_tr_test$Exposure))

# Printing the probabilities
print(prob_glm)
print(prob_xgb)

```

We see that this somewhat makes sense since all our predicted numbers are concentrated on 0. So we try another wat to see the two-step procedure:

1) Binary classification - if there ClaimNb>0 or not
2) Given ClaimNb>0, get probabilities when ClaimNb = 0-4

Then treat 2) as a conditional probability, multiplied by 1) and finally obtain the overall probability.

## Two-step Procedure
### Binary probabilities

We do this binary classification according to our above binary classifier (which is constructed by setting threshold)

```{r}
# Binary PMF from binary predictions
calc_binary_pmf <- function(binary_predictions) {

  counts <- table(binary_predictions)
  total_count <- length(binary_predictions)

  prob_0 <- counts['0'] / total_count
  prob_1 <- counts['1'] / total_count

  if (is.na(prob_0)) prob_0 <- 0
  if (is.na(prob_1)) prob_1 <- 0
  
  return(c(prob_0 = prob_0, prob_1 = prob_1))
}

# Binary PMF for GLM and XGBoost models
binary_pmf_glm <- calc_binary_pmf(binary_predictions_glm)
binary_pmf_xgb <- calc_binary_pmf(binary_predictions_xgb)

print("Binary PMF for GLM:")
print(binary_pmf_glm)
print("Binary PMF for XGBoost:")
print(binary_pmf_xgb)

```

### XGBoost For Given non-zero claims

```{r}
# Filter training data for non-zero claims
train_nonzero <- info_tr_train[info_tr_train$ClaimNb_cap / info_tr_train$Exposure > 0, ]
train_nonzero$Scaled_ClaimNb <- train_nonzero$ClaimNb_cap /train_nonzero$Exposure
train_nonzero$log_Scaled_ClaimNb <- log(train_nonzero$Scaled_ClaimNb + 1)

# Filter testing data for non-zero claims
test_nonzero <- info_tr_test[info_tr_test$ClaimNb_cap / info_tr_test$Exposure > 0, ]
test_nonzero$Scaled_ClaimNb <- test_nonzero$ClaimNb_cap /test_nonzero$Exposure
test_nonzero$log_Scaled_ClaimNb <- log(test_nonzero$Scaled_ClaimNb + 1)

train_nonzero$VehGas <- as.factor(train_nonzero$VehGas)
test_nonzero$VehGas <- as.factor(test_nonzero$VehGas)

categorical_features <- sapply(train_nonzero, is.factor)
train_nonzero[categorical_features] <- lapply(train_nonzero[categorical_features], function(x) as.integer(as.factor(x)))

categorical_features <- sapply(test_nonzero, is.factor)
test_nonzero[categorical_features] <- lapply(test_nonzero[categorical_features], function(x) as.integer(as.factor(x)))

X_train_nonzero <- as.matrix(train_nonzero[, -c(1,2,12,13,14)]) 
y_train_nonzero <- train_nonzero$log_Scaled_ClaimNb

X_test_nonzero <- as.matrix(test_nonzero[, -c(1,2,12,13,14)])
y_test_nonzero <- test_nonzero$log_Scaled_ClaimNb

dtrain_nonzero <- xgb.DMatrix(data = X_train_nonzero, label = y_train_nonzero)
dtest_nonzero <- xgb.DMatrix(data = X_test_nonzero, label = y_test_nonzero)

# XGBoost model parameters
params_nonzero <- list(
  booster = "gbtree",
  objective = "reg:squaredlogerror", 
  eta = 0.1,
  max_depth = 6,
  subsample = 0.5,
  colsample_bytree = 0.7
)

xgb_model_nonzero <- xgboost(params = params_nonzero, data = dtrain_nonzero, nrounds = 100, verbose = 0)

```

```{r}
predictions_nonzero <- predict(xgb_model_nonzero, dtest_nonzero)

predictions_nonzero <- exp(predictions_nonzero) - 1
predictions_nonzero <- predictions_nonzero * test_nonzero$Exposure

summary(predictions_nonzero)
hist(predictions_nonzero)
```

```{r}
# Round predictions
rounded_predictions_nonzero <- round(predictions_nonzero)
```

```{r}
pmf_counts <- table(rounded_predictions_nonzero)
pmf_total <- sum(pmf_counts)

pmf_probabilities <- pmf_counts / pmf_total

# Ensure we have probabilities for 0 through 4
required_values <- 0:4
nonzero_pmf_probabilities <- sapply(required_values, function(x) {
  if(x %in% names(pmf_probabilities)) {
    as.numeric(pmf_probabilities[as.character(x)])
  } else {
    0
  }
})

names(nonzero_pmf_probabilities) <- required_values

print(nonzero_pmf_probabilities)

```

### Total PMF

```{r}
overall_pmf <- numeric(5)  

overall_pmf[1] <- binary_pmf_xgb[1] 

if(length(nonzero_pmf_probabilities) > 0) {
  for (k in 1:4) {
    if(k <= length(nonzero_pmf_probabilities)) {
      overall_pmf[k + 1] <- binary_pmf_xgb[2] * nonzero_pmf_probabilities[k]
    } else {
      overall_pmf[k + 1] <- 0 
    }
  }
}

names(overall_pmf) <- 0:4 
print(overall_pmf)

```


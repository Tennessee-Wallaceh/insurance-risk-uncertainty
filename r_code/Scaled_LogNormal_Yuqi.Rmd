---
title: "Regression for Scaled Data"
author: "Yuqi Zhang"
date: "2024-06-10"
output: pdf_document
---

```{r}
scaled_data_train <- cbind(info_tr_train$ClaimNb_cap/info_tr_train$Exposure,info_tr_train)
colnames(scaled_data_train) <- c("Scaled_ClaimNb",colnames(info_tr_train))
scaled_data_train <- scaled_data_train[, -c(2,3,13)]

scaled_data_test <- cbind(info_tr_test$ClaimNb_cap/info_tr_test$Exposure,info_tr_test)
colnames(scaled_data_test) <- c("Scaled_ClaimNb",colnames(info_tr_test))
scaled_data_test <- scaled_data_test[, -c(2,3,13)]
```

```{r}
# log-transformation
scaled_data_train$log_Scaled_ClaimNb <- log(scaled_data_train$Scaled_ClaimNb + 1)
#scaled_data_train <- #scaled_data_train[scaled_data_train$Scaled_ClaimNb < quantile(scaled_data_train$Scaled_ClaimNb, 0.99),]

# Categorical to factors
scaled_data_train$Area <- as.factor(scaled_data_train$Area)
scaled_data_train$VehBrand <- as.factor(scaled_data_train$VehBrand)
scaled_data_train$VehGas <- as.factor(scaled_data_train$VehGas)
scaled_data_train$Region <- as.factor(scaled_data_train$Region)

scaled_data_train <- scaled_data_train[, -c(1)]

# log-transformation
scaled_data_test$log_Scaled_ClaimNb <- log(scaled_data_test$Scaled_ClaimNb + 1)
#scaled_data_test <- #scaled_data_test[scaled_data_test$Scaled_ClaimNb < quantile(scaled_data_test$Scaled_ClaimNb, 0.99),]

# Categorical to factors
scaled_data_test$Area <- as.factor(scaled_data_test$Area)
scaled_data_test$VehBrand <- as.factor(scaled_data_test$VehBrand)
scaled_data_test$VehGas <- as.factor(scaled_data_test$VehGas)
scaled_data_test$Region <- as.factor(scaled_data_test$Region)

scaled_data_test <- scaled_data_test[, -c(1)]
```


```{r}
library(nnet)
library(MASS)

# GLM
formula <- log_Scaled_ClaimNb ~as.factor(Area) + VehPower + VehAge + DrivAge + BonusMalus + as.factor(VehBrand) + as.factor(VehGas) + as.factor(Region) + Density

glm_model <- glm(formula, data = scaled_data_train, family = gaussian())
```



```{r}
library(caret)
library(xgboost)

X_train <- scaled_data_train[, -c(10)]
y_train <- scaled_data_train$log_Scaled_ClaimNb
X_test <- scaled_data_test[, -c(10)]
y_test <- scaled_data_test$log_Scaled_ClaimNb

categorical_features <- sapply(X_train, is.factor)
X_train[categorical_features] <- lapply(X_train[categorical_features], function(x) as.integer(as.factor(x)))

categorical_features <- sapply(X_test, is.factor)
X_test[categorical_features] <- lapply(X_test[categorical_features], function(x) as.integer(as.factor(x)))

X_train <- as.matrix(X_train)
X_test <- as.matrix(X_test)

# Converting to DMatrix for XGBoost
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest <- xgb.DMatrix(data = X_test, label = y_test)

```

```{r}
# XGBoost model parameters
params <- list(
  booster = "gbtree",
  objective = "reg:squaredlogerror",
  eta = 0.1,
  max_depth = 6,
  subsample = 0.5,
  colsample_bytree = 0.7
)

# XGBoost model
xgb_model <- xgboost(params = params, data = dtrain, nrounds = 100, verbose = 0)

```


```{r}
calculate_gini <- function(actual, predicted) {

  df <- data.frame(actual = actual, predicted = predicted)
  df <- df[order(df$predicted),]

  # Calculate the cumulative sums of actual values
  cum_actuals <- cumsum(df$actual) / sum(df$actual)
  cum_predicted = cumsum(df$predicted) / sum(df$predicted)
  
  # Area under the Lorenz curve
  Lorenz = cumsum(sort(df$actual) / sum(df$actual))
  B = sum(Lorenz[-length(Lorenz)]) / (length(Lorenz) - 1)

  # Area above the Lorenz curve
  A = 0.5 - B
  gini = (A / 0.5)

  return(gini)
}

```

```{r}
# GLM
predictions_glm <- exp(predict(glm_model, newdata = scaled_data_test, type = "response")) - 1

hist(predictions_glm)

# MSE for GLM
mse_glm <- mean((predictions_glm - info_tr_test$ClaimNb_cap/info_tr_test$Exposure)^2)
print(paste("GLM MSE:", mse_glm))

# Gini for GLM
gini_glm <- calculate_gini(info_tr_test$ClaimNb_cap/info_tr_test$Exposure, predictions_glm)
print(paste("GLM Gini:", gini_glm))

```

```{r}
# XGBoost
predictions_xgb <- predict(xgb_model, dtest)
predictions_xgb <- exp(predictions_xgb) - 1
hist(predictions_xgb)

# MSE for XGBoost
mse_xgb <- mean((predictions_xgb - info_tr_test$ClaimNb_cap/info_tr_test$Exposure)^2)
print(paste("XGBoost MSE:", mse_xgb))

# Gini for XGBoost
gini_xgb <- calculate_gini(info_tr_test$ClaimNb_cap/info_tr_test$Exposure, predictions_xgb)
print(paste("XGBoost Gini:", gini_xgb))

```
The very high Gini coefficients and show that GLM and XGBoost both effectively discriminate between different outcomes. It ranks the predictions very well relative to the actual data.


```{r}
# Reverse transformation log(Scaled_ClaimNb + 1)
predicted_scaled_claim_nb_glm <- exp(predictions_glm) - 1
predicted_scaled_claim_nb_xgb <- exp(predictions_xgb) - 1

hist(predicted_scaled_claim_nb_glm)
hist(predicted_scaled_claim_nb_xgb)

predicted_claimNb_glm <- predicted_scaled_claim_nb_glm * info_tr_test$Exposure
predicted_claimNb_xgb <- predicted_scaled_claim_nb_xgb * info_tr_test$Exposure

hist(predicted_claimNb_glm)
hist(predicted_claimNb_xgb)

```

```{r}
prob_claimNb_xgb <- 1 - exp(-predicted_claimNb_xgb)
prob_claimNb_xgb
```


```{r}
# Binary classification of predictions
binary_predictions_glm <- ifelse(predicted_claimNb_glm > 0.05, 1, 0)
binary_predictions_xgb <- ifelse(predicted_claimNb_xgb > 0.05, 1, 0)

# Actual binary values
binary_actuals <- ifelse(info_test$ClaimNb_cap > 0, 1, 0)

```

```{r}
length(binary_predictions_glm)
length(binary_predictions_xgb)
length(binary_actuals)
```

```{r}
library(caret)

# Confusion matrix for GLM
confusion_matrix_glm <- confusionMatrix(factor(binary_predictions_glm), factor(binary_actuals))
print(confusion_matrix_glm)

# Confusion matrix for XGBoost
confusion_matrix_xgb <- confusionMatrix(factor(binary_predictions_xgb), factor(binary_actuals))
print(confusion_matrix_xgb)

mean((binary_predictions_glm-binary_actuals)^2)
mean((binary_predictions_xgb-binary_actuals)^2)

calculate_gini(predicted_claimNb_glm, info_tr_test$ClaimNb_cap)
calculate_gini(predicted_claimNb_xgb, info_tr_test$ClaimNb_cap)
```


```{r}
min_length <- min(length(binary_predictions_glm), length(binary_predictions_xgb), length(binary_actuals))

data_for_plot <- data.frame(
  Actual = binary_actuals[1:min_length],
  GLM_Predictions = binary_predictions_glm[1:min_length],
  XGB_Predictions = binary_predictions_xgb[1:min_length]
)

library(reshape2)

melted_data <- melt(data_for_plot, id.vars = NULL, variable.name = "Model", value.name = "Claim Status")

```
```{r}
library(ggplot2)

ggplot(melted_data, aes(x = as.factor(`Claim Status`), fill = Model)) +
  geom_bar(position = "dodge", stat="count") +
  scale_x_discrete(labels = c("Zero", "Non-Zero")) + 
  labs(title = "Comparison of Actual and Predicted Claims (Binary)",
       x = "Claim Status",
       y = "Count",
       fill = "Model Type") +
  facet_wrap(~ Model, scales = "free_y") + 
  theme_minimal()


```


```{r}
nonzero_indices <- which(info_tr_test$ClaimNb_cap/info_tr_test$Exposure > 0)

mse_nonzero_glm <- mean((predictions_glm[nonzero_indices] - (info_tr_test$ClaimNb_cap/info_tr_test$Exposure)[nonzero_indices])^2)

mse_nonzero_xgb <- mean((predictions_xgb[nonzero_indices] - (info_tr_test$ClaimNb_cap/info_tr_test$Exposure)[nonzero_indices])^2)

print(paste("GLM MSE for Nonzero Claims:", mse_nonzero_glm))
print(paste("XGBoost MSE for Nonzero Claims:", mse_nonzero_xgb))

```


```{r}
library(stats)

# Calculate probabilities
calc_lognormal_probs <- function(predictions, exposure) {
  probs <- sapply(0:4, function(k) {
    lower_bound <- k / exposure
    upper_bound <- (k + 1) / exposure
    p_lower <- plnorm(lower_bound, meanlog = mean(predictions), sdlog = sd(predictions))
    p_upper <- plnorm(upper_bound, meanlog = mean(predictions), sdlog = sd(predictions))
    p_upper - p_lower
  })
  return(probs)
}

# Calculate probabilities for GLM and XGBoost models
prob_glm <- calc_lognormal_probs(predictions_glm, mean(info_tr_test$Exposure))
prob_xgb <- calc_lognormal_probs(predictions_xgb, mean(info_tr_test$Exposure))

# Printing the probabilities
print(prob_glm)
print(prob_xgb)

```

## Binary probabilities

```{r}
# Binary PMF from binary predictions
calc_binary_pmf <- function(binary_predictions) {

  counts <- table(binary_predictions)
  total_count <- length(binary_predictions)

  prob_0 <- counts['0'] / total_count
  prob_1 <- counts['1'] / total_count

  if (is.na(prob_0)) prob_0 <- 0
  if (is.na(prob_1)) prob_1 <- 0
  
  return(c(prob_0 = prob_0, prob_1 = prob_1))
}

# Binary PMF for GLM and XGBoost models
binary_pmf_glm <- calc_binary_pmf(binary_predictions_glm)
binary_pmf_xgb <- calc_binary_pmf(binary_predictions_xgb)

print("Binary PMF for GLM:")
print(binary_pmf_glm)
print("Binary PMF for XGBoost:")
print(binary_pmf_xgb)

```

## For non-zero claims

```{r}
# Filter training data for non-zero claims
train_nonzero <- info_tr_train[info_tr_train$ClaimNb_cap / info_tr_train$Exposure > 0, ]
train_nonzero$Scaled_ClaimNb <- train_nonzero$ClaimNb_cap /train_nonzero$Exposure
train_nonzero$log_Scaled_ClaimNb <- log(train_nonzero$Scaled_ClaimNb + 1)

# Filter testing data for non-zero claims
test_nonzero <- info_tr_test[info_tr_test$ClaimNb_cap / info_tr_test$Exposure > 0, ]
test_nonzero$Scaled_ClaimNb <- test_nonzero$ClaimNb_cap /test_nonzero$Exposure
test_nonzero$log_Scaled_ClaimNb <- log(test_nonzero$Scaled_ClaimNb + 1)

train_nonzero$VehGas <- as.factor(train_nonzero$VehGas)
test_nonzero$VehGas <- as.factor(test_nonzero$VehGas)

categorical_features <- sapply(train_nonzero, is.factor)
train_nonzero[categorical_features] <- lapply(train_nonzero[categorical_features], function(x) as.integer(as.factor(x)))

categorical_features <- sapply(test_nonzero, is.factor)
test_nonzero[categorical_features] <- lapply(test_nonzero[categorical_features], function(x) as.integer(as.factor(x)))

X_train_nonzero <- as.matrix(train_nonzero[, -c(1,2,12,13,14)]) 
y_train_nonzero <- train_nonzero$log_Scaled_ClaimNb

X_test_nonzero <- as.matrix(test_nonzero[, -c(1,2,12,13,14)])
y_test_nonzero <- test_nonzero$log_Scaled_ClaimNb

```

```{r}
dtrain_nonzero <- xgb.DMatrix(data = X_train_nonzero, label = y_train_nonzero)
dtest_nonzero <- xgb.DMatrix(data = X_test_nonzero, label = y_test_nonzero)

# XGBoost model parameters
params_nonzero <- list(
  booster = "gbtree",
  objective = "reg:squaredlogerror", 
  eta = 0.1,
  max_depth = 6,
  subsample = 0.5,
  colsample_bytree = 0.7
)

xgb_model_nonzero <- xgboost(params = params_nonzero, data = dtrain_nonzero, nrounds = 100, verbose = 0)

```

```{r}
predictions_nonzero <- predict(xgb_model_nonzero, dtest_nonzero)

predictions_nonzero <- exp(predictions_nonzero) - 1
predictions_nonzero <- predictions_nonzero * test_nonzero$Exposure

summary(predictions_nonzero)
hist(predictions_nonzero)

```

```{r}
# Round predictions
rounded_predictions_nonzero <- round(predictions_nonzero)
```

```{r}
pmf_counts <- table(rounded_predictions_nonzero)
pmf_total <- sum(pmf_counts)

pmf_probabilities <- pmf_counts / pmf_total

# Ensure we have probabilities for 0 through 4
required_values <- 0:4
nonzero_pmf_probabilities <- sapply(required_values, function(x) {
  if(x %in% names(pmf_probabilities)) {
    as.numeric(pmf_probabilities[as.character(x)])
  } else {
    0
  }
})

names(nonzero_pmf_probabilities) <- required_values

print(nonzero_pmf_probabilities)

```

## Total PMF

```{r}
overall_pmf <- numeric(5)  

overall_pmf[1] <- binary_pmf_xgb[1] 

if(length(nonzero_pmf_probabilities) > 0) {
  for (k in 1:4) {
    if(k <= length(nonzero_pmf_probabilities)) {
      overall_pmf[k + 1] <- binary_pmf_xgb[2] * nonzero_pmf_probabilities[k]
    } else {
      overall_pmf[k + 1] <- 0 
    }
  }
}

names(overall_pmf) <- 0:4 
print(overall_pmf)

```


---
title: "Regression for Scaled Data"
author: "Yuqi Zhang"
date: "2024-06-10"
output: pdf_document
---

```{r}
scaled_data_train <- cbind(info_tr_train$ClaimNb_cap/info_tr_train$Exposure,info_tr_train)
colnames(scaled_data_train) <- c("Scaled_ClaimNb",colnames(info_tr_train))
scaled_data_train <- scaled_data_train[, -c(2,3,13)]

scaled_data_test <- cbind(info_tr_test$ClaimNb_cap/info_tr_test$Exposure,info_tr_test)
colnames(scaled_data_test) <- c("Scaled_ClaimNb",colnames(info_tr_test))
scaled_data_test <- scaled_data_test[, -c(2,3,13)]
```

```{r}
# log-transformation
scaled_data_train$log_Scaled_ClaimNb <- log(scaled_data_train$Scaled_ClaimNb + 1)
scaled_data_train <- scaled_data_train[scaled_data_train$Scaled_ClaimNb < quantile(scaled_data_train$Scaled_ClaimNb, 0.99),]

# Categorical to factors
scaled_data_train$Area <- as.factor(scaled_data_train$Area)
scaled_data_train$VehBrand <- as.factor(scaled_data_train$VehBrand)
scaled_data_train$VehGas <- as.factor(scaled_data_train$VehGas)
scaled_data_train$Region <- as.factor(scaled_data_train$Region)

scaled_data_train <- scaled_data_train[, -c(1)]

# log-transformation
scaled_data_test$log_Scaled_ClaimNb <- log(scaled_data_test$Scaled_ClaimNb + 1)
scaled_data_test <- scaled_data_test[scaled_data_test$Scaled_ClaimNb < quantile(scaled_data_test$Scaled_ClaimNb, 0.99),]

# Categorical to factors
scaled_data_test$Area <- as.factor(scaled_data_test$Area)
scaled_data_test$VehBrand <- as.factor(scaled_data_test$VehBrand)
scaled_data_test$VehGas <- as.factor(scaled_data_test$VehGas)
scaled_data_test$Region <- as.factor(scaled_data_test$Region)

scaled_data_test <- scaled_data_test[, -c(1)]
```


```{r}
library(nnet)
library(MASS)

# GLM
formula <- log_Scaled_ClaimNb ~as.factor(Area) + VehPower + VehAge + DrivAge + BonusMalus + as.factor(VehBrand) + as.factor(VehGas) + as.factor(Region) + Density

glm_model <- glm(formula, data = scaled_data_train, family = gaussian())
```



```{r}
library(caret)
library(xgboost)

X_train <- scaled_data_train[, -c(10)]
y_train <- scaled_data_train$log_Scaled_ClaimNb
X_test <- scaled_data_test[, -c(10)]
y_test <- scaled_data_test$log_Scaled_ClaimNb

categorical_features <- sapply(X_train, is.factor)
X_train[categorical_features] <- lapply(X_train[categorical_features], function(x) as.integer(as.factor(x)))

categorical_features <- sapply(X_test, is.factor)
X_test[categorical_features] <- lapply(X_test[categorical_features], function(x) as.integer(as.factor(x)))

X_train <- as.matrix(X_train)
X_test <- as.matrix(X_test)

# Converting to DMatrix for XGBoost
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest <- xgb.DMatrix(data = X_test, label = y_test)

```

```{r}
# XGBoost model parameters
params <- list(
  booster = "gbtree",
  objective = "reg:squaredlogerror",
  eta = 0.1,
  max_depth = 6,
  subsample = 0.5,
  colsample_bytree = 0.7
)

# XGBoost model
xgb_model <- xgboost(params = params, data = dtrain, nrounds = 100, verbose = 0)

```


```{r}
calculate_gini <- function(actual, predicted) {

  df <- data.frame(actual = actual, predicted = predicted)
  df <- df[order(df$predicted),]

  # Calculate the cumulative sums of actual values
  cum_actuals <- cumsum(df$actual) / sum(df$actual)
  cum_predicted = cumsum(df$predicted) / sum(df$predicted)
  
  # Area under the Lorenz curve
  Lorenz = cumsum(sort(df$actual) / sum(df$actual))
  B = sum(Lorenz[-length(Lorenz)]) / (length(Lorenz) - 1)

  # Area above the Lorenz curve
  A = 0.5 - B
  gini = (A / 0.5)

  return(gini)
}

```

```{r}
# GLM
predictions_glm <- exp(predict(glm_model, newdata = scaled_data_test, type = "response")) - 1

hist(predictions_glm)

# MSE for GLM
mse_glm <- mean((predictions_glm - scaled_data_test$log_Scaled_ClaimNb)^2)
print(paste("GLM MSE:", mse_glm))

# Gini for GLM
gini_glm <- calculate_gini(scaled_data_test$log_Scaled_ClaimNb, predictions_glm)
print(paste("GLM Gini:", gini_glm))

```

```{r}
# XGBoost
predictions_xgb <- predict(xgb_model, dtest)
hist(predictions_xgb)

# MSE for XGBoost
mse_xgb <- mean((predictions_xgb - y_test)^2)
print(paste("XGBoost MSE:", mse_xgb))

# Gini for XGBoost
gini_xgb <- calculate_gini(y_test, predictions_xgb)
print(paste("XGBoost Gini:", gini_xgb))

```
The very high Gini coefficients and show that GLM and XGBoost both effectively discriminate between different outcomes. It ranks the predictions very well relative to the actual data.




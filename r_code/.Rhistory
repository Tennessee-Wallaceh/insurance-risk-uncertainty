library(dplyr)
library(tidyr)
# Data Load ----
url <- "https://github.com/dutangc/CASdatasets/blob/master/data/freMTPL2freq.rda?raw=true"
load(url(url))
url <- "https://github.com/dutangc/CASdatasets/blob/master/data/freMTPL2sev.rda?raw=true"
load(url(url))
#########################################################################################################################
# Will have 2 sets of data: Original, Transformed features #
#########################################################################################################################
# For each set we will have 3 data objects:
# info - dataset to train the number of claims model
# comb - dataset to train the amount of a claim model
# comb_agg - dataset to test model for modelling the total amount for a policy
# Then to split the datasets we split the info and comb dataset
# Then in order to get the conformal intervals and check the coverage for the total claims we use the same split as used for the info set
# Using stratified sampling to generate the 3 sets
#### Original ####
freMTPL2freq$Exposure = pmin(freMTPL2freq$Exposure, 1)
info = freMTPL2freq ; claims = freMTPL2sev
# rm(freMTPL2freq);rm(freMTPL2sev)
# removing outliers
# for the info dataset truncated the number of claims to 4
info$ClaimNb_cap = pmin(info$ClaimNb, 4)
info = select(info, -"ClaimNb")
# for the claims dataset have removed the top 5% of values
claims = claims[claims$ClaimAmount < quantile(claims$ClaimAmount, 0.95),]
# Need to correct the number of claims for each policy now for the combined and aggragated dataset.
claims = claims %>%
group_by(IDpol) %>%
mutate(ClaimNb = n()) %>%
ungroup()
comb = merge(x = info, y = claims, by = "IDpol", all.x = TRUE)
comb$ClaimAmount[is.na(comb$ClaimAmount)] = 0
comb = select(comb, -"ClaimNb_cap")
comb$ClaimNb = ifelse(is.na(comb$ClaimNb), 0, comb$ClaimNb)
comb$ClaimNb_cap = pmin(comb$ClaimNb, 4)
comb_agg = comb %>%
group_by(IDpol) %>%
summarise(ClaimAmount = sum(ClaimAmount))
comb = comb[!comb$ClaimAmount == 0,]
# stratified sampling
# info dataset
zeroind = which(info$ClaimNb == 0)
nonzeroind = which(info$ClaimNb > 0)
set.seed(100)
traincalzeroind = sample(zeroind, floor(0.8*length(zeroind)))
# testzero = info[-traincalzeroind,]
testzero = info[setdiff(zeroind,traincalzeroind),]
calzeroind = sample(traincalzeroind, floor(0.0625*length(zeroind)))
calzero = info[calzeroind,]
trainzero = info[setdiff(traincalzeroind,calzeroind),]
traincalnzind = sample(nonzeroind, floor(0.8*length(nonzeroind)))
# testnz = info[-traincalnzind,]
testnz = info[setdiff(nonzeroind,traincalnzind),]
calnzind = sample(traincalnzind, floor(0.0625*length(nonzeroind)))
calnz = info[calnzind,]
trainnz = info[setdiff(traincalnzind,calnzind),]
info_train = rbind(trainzero, trainnz)
info_cal = rbind(calzero, calnz)
info_test = rbind(testzero, testnz)
# comb dataset, not stratified sampling as only using data with non zero claims
set.seed(100)
traincalind = sample(1:nrow(comb), floor(0.8*(nrow(comb))))
comb_test = comb[-traincalind,]
calind = sample(traincalind, floor(0.0625*nrow(comb)))
comb_cal = comb[calind,]
comb_train = comb[setdiff(traincalind,calind),]
##########################################################################################################################################################################
#### Transformed features ####
info_tr = freMTPL2freq; claims_tr = freMTPL2sev
# Feature pre-processing for GLM regression
info_tr$Area <- as.integer(info_tr$Area)
# truncating vehicle power to 9
info_tr$VehPower <- as.factor(pmin(info_tr$VehPower,9))
# Creating 3 groups for the Vehicl age
VehAge <- cbind(c(0:110), c(1, rep(2,10), rep (3,100)))
info_tr$VehAge <- as.factor(VehAge[info_tr$VehAge+1,2])
info_tr[,"VehAge"] <- relevel(info_tr[,"VehAge"], ref="2")
# Creating 7 groups for the driver age
DrivAge <- cbind(
c(18:100),  # Age vector from 18 to 100
c(
rep(1, 4),    # Ages 18-21 (4 values)
rep(2, 5),    # Ages 22-26 (5 values)
rep(3, 5),    # Ages 27-31 (5 values)
rep(4, 10),   # Ages 32-41 (10 values)
rep(5, 10),   # Ages 42-51 (10 values)
rep(6, 20),   # Ages 52-71 (20 values)
rep(7, 29)    # Ages 72-100 (29 values)
)
)
info_tr$DrivAge <- as.factor(DrivAge[info$DrivAge-17,2])
info_tr[,"DrivAge"] <- relevel(info_tr[,"DrivAge"], ref = "5")
# trunacating BonusMalus to 150
info_tr$BonusMalus <- as.integer(pmin(info_tr$BonusMalus, 150))
# log transform for density
info_tr$Density <- as.numeric(log(info_tr$Density))
info_tr[,"Region"] <- relevel(info_tr[,"Region"], ref="Centre")
##################
info_tr$ClaimNb_cap = pmin(info_tr$ClaimNb, 4)
info_tr = select(info_tr, -"ClaimNb")
# for the claims dataset have removed the top 5% of values
claims_tr = claims_tr[claims_tr$ClaimAmount < quantile(claims_tr$ClaimAmount, 0.95),]
# Need to correct the number of claims for each policy now for the combined and aggragated dataset.
claims_tr = claims_tr %>%
group_by(IDpol) %>%
mutate(ClaimNb = n()) %>%
ungroup()
comb_tr = merge(x = info_tr, y = claims_tr, by = "IDpol", all.x = TRUE)
comb_tr$ClaimAmount[is.na(comb_tr$ClaimAmount)] = 0
comb_tr = select(comb_tr, -"ClaimNb_cap")
comb_tr$ClaimNb = ifelse(is.na(comb_tr$ClaimNb), 0, comb_tr$ClaimNb)
comb_tr$ClaimNb_cap = pmin(comb_tr$ClaimNb, 4)
comb_agg_tr = comb_tr %>%
group_by(IDpol) %>%
summarise(ClaimAmount = sum(ClaimAmount))
comb_tr = comb_tr[!comb_tr$ClaimAmount == 0,]
# stratified sampling
# info dataset
info_tr$VehPower = as.numeric(info_tr$VehPower)
info_tr$VehAge = as.numeric(info_tr$VehAge)
info_tr$DrivAge = as.numeric(info_tr$DrivAge)
testzero = info_tr[setdiff(zeroind,traincalzeroind),]
calzero = info_tr[calzeroind,]
trainzero = info_tr[setdiff(traincalzeroind,calzeroind),]
testnz = info_tr[setdiff(nonzeroind,traincalnzind),]
calnz = info_tr[calnzind,]
trainnz = info_tr[setdiff(traincalnzind,calnzind),]
info_tr_train = rbind(trainzero, trainnz)
info_tr_cal = rbind(calzero, calnz)
info_tr_test = rbind(testzero, testnz)
# comb dataset, not stratified sampling as only using data with non zero claims
comb_tr_test = comb_tr[-traincalind,]
comb_tr_cal = comb_tr[calind,]
comb_tr_train = comb_tr[setdiff(traincalind,calind),]
scaled_data_train <- cbind(info_tr_train$ClaimNb_cap/info_tr_train$Exposure,info_tr_train)
colnames(scaled_data_train) <- c("Scaled_ClaimNb",colnames(info_tr_train))
scaled_data_train <- scaled_data_train[, -c(2,3,13)]
scaled_data_test <- cbind(info_tr_test$ClaimNb_cap/info_tr_test$Exposure,info_tr_test)
colnames(scaled_data_test) <- c("Scaled_ClaimNb",colnames(info_tr_test))
scaled_data_test <- scaled_data_test[, -c(2,3,13)]
# log-transformation
scaled_data_train$log_Scaled_ClaimNb <- log(scaled_data_train$Scaled_ClaimNb + 1)
#scaled_data_train <- #scaled_data_train[scaled_data_train$Scaled_ClaimNb < quantile(scaled_data_train$Scaled_ClaimNb, 0.99),]
# Categorical to factors
scaled_data_train$Area <- as.factor(scaled_data_train$Area)
scaled_data_train$VehBrand <- as.factor(scaled_data_train$VehBrand)
scaled_data_train$VehGas <- as.factor(scaled_data_train$VehGas)
scaled_data_train$Region <- as.factor(scaled_data_train$Region)
scaled_data_train <- scaled_data_train[, -c(1)]
# log-transformation
scaled_data_test$log_Scaled_ClaimNb <- log(scaled_data_test$Scaled_ClaimNb + 1)
#scaled_data_test <- #scaled_data_test[scaled_data_test$Scaled_ClaimNb < quantile(scaled_data_test$Scaled_ClaimNb, 0.99),]
# Categorical to factors
scaled_data_test$Area <- as.factor(scaled_data_test$Area)
scaled_data_test$VehBrand <- as.factor(scaled_data_test$VehBrand)
scaled_data_test$VehGas <- as.factor(scaled_data_test$VehGas)
scaled_data_test$Region <- as.factor(scaled_data_test$Region)
scaled_data_test <- scaled_data_test[, -c(1)]
library(nnet)
library(MASS)
# GLM
formula <- log_Scaled_ClaimNb ~as.factor(Area) + VehPower + VehAge + DrivAge + BonusMalus + as.factor(VehBrand) + as.factor(VehGas) + as.factor(Region) + Density
glm_model <- glm(formula, data = scaled_data_train, family = gaussian())
library(caret)
library(xgboost)
X_train <- scaled_data_train[, -c(10)]
y_train <- scaled_data_train$log_Scaled_ClaimNb
X_test <- scaled_data_test[, -c(10)]
y_test <- scaled_data_test$log_Scaled_ClaimNb
categorical_features <- sapply(X_train, is.factor)
X_train[categorical_features] <- lapply(X_train[categorical_features], function(x) as.integer(as.factor(x)))
categorical_features <- sapply(X_test, is.factor)
X_test[categorical_features] <- lapply(X_test[categorical_features], function(x) as.integer(as.factor(x)))
X_train <- as.matrix(X_train)
X_test <- as.matrix(X_test)
# Converting to DMatrix for XGBoost
dtrain <- xgb.DMatrix(data = X_train, label = y_train)
dtest <- xgb.DMatrix(data = X_test, label = y_test)
# XGBoost model parameters
params <- list(
booster = "gbtree",
objective = "reg:squaredlogerror",
eta = 0.1,
max_depth = 6,
subsample = 0.5,
colsample_bytree = 0.7
)
# XGBoost model
xgb_model <- xgboost(params = params, data = dtrain, nrounds = 100, verbose = 0)
calculate_gini <- function(actual, predicted) {
df <- data.frame(actual = actual, predicted = predicted)
df <- df[order(df$predicted),]
# Calculate the cumulative sums of actual values
cum_actuals <- cumsum(df$actual) / sum(df$actual)
cum_predicted = cumsum(df$predicted) / sum(df$predicted)
# Area under the Lorenz curve
Lorenz = cumsum(sort(df$actual) / sum(df$actual))
B = sum(Lorenz[-length(Lorenz)]) / (length(Lorenz) - 1)
# Area above the Lorenz curve
A = 0.5 - B
gini = (A / 0.5)
return(gini)
}
# GLM
predictions_glm <- exp(predict(glm_model, newdata = scaled_data_test, type = "response")) - 1
hist(predictions_glm)
# MSE for GLM
mse_glm <- mean((predictions_glm - info_tr_test$ClaimNb_cap/info_tr_test$Exposure)^2)
print(paste("GLM MSE:", mse_glm))
# Gini for GLM
gini_glm <- calculate_gini(info_tr_test$ClaimNb_cap/info_tr_test$Exposure, predictions_glm)
print(paste("GLM Gini:", gini_glm))
# XGBoost
predictions_xgb <- predict(xgb_model, dtest)
predictions_xgb <- exp(predictions_xgb) - 1
hist(predictions_xgb)
# MSE for XGBoost
mse_xgb <- mean((predictions_xgb - info_tr_test$ClaimNb_cap/info_tr_test$Exposure)^2)
print(paste("XGBoost MSE:", mse_xgb))
# Gini for XGBoost
gini_xgb <- calculate_gini(info_tr_test$ClaimNb_cap/info_tr_test$Exposure, predictions_xgb)
print(paste("XGBoost Gini:", gini_xgb))
predictions_glm_non_neg <- pmax(predictions_glm, 0)
predictions_xgb_non_neg <- pmax(predictions_xgb, 0)
# Convert these to probabilities assuming a logistic model
probabilities_glm <- 1 / (1 + exp(-predictions_glm_non_neg))
probabilities_xgb <- 1 / (1 + exp(-predictions_xgb_non_neg))
hist(probabilities_glm)
hist(probabilities_xgb)
hist(probabilities_glm)
hist(probabilities_xgb)
# Binary predictions using a threshold (e.g., 0.5 for logistic)
binary_predictions_glm_prob <- ifelse(probabilities_glm > 0.51, 1, 0)
binary_predictions_xgb_prob <- ifelse(probabilities_xgb > 0.51, 1, 0)
# Printing the probabilities and binary predictions
print(probabilities_glm)
print(probabilities_xgb)
print(binary_predictions_glm_prob)
print(binary_predictions_xgb_prob)
hist(binary_predictions_glm_prob)
hist(probabilities_glm)
hist(probabilities_xgb)
hist(binary_predictions_glm_prob)
hist(binary_predictions_xgb_prob)
hist(probabilities_glm)
hist(probabilities_xgb)
binary_predictions_xgb_prob <- ifelse(probabilities_xgb > 0.53, 1, 0)
hist(binary_predictions_xgb_prob)
binary_predictions_xgb_prob <- ifelse(probabilities_xgb > 0.525, 1, 0)
hist(binary_predictions_xgb_prob)
binary_predictions_xgb_prob <- ifelse(probabilities_xgb > 0.52, 1, 0)
# Normalize function
normalise <- function(x) {
(x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}
# Calculate predictions for GLM and XGBoost
predictions_glm <- exp(predict(glm_model, newdata = scaled_data_test, type = "response")) - 1
predictions_xgb <- exp(predict(xgb_model, dtest)) - 1
# Normalize predictions to get 'probabilities'
probabilities_glm <- normalise(predictions_glm)
probabilities_xgb <- normalise(predictions_xgb)
# Output probabilities
print(probabilities_glm)
print(probabilities_xgb)
hist(probabilities_glm)
hist(probabilities_xgb)
# Logistic transformation
logistic <- function(x) {
1 / (1 + exp(-x))
}
# Calculate predictions for GLM and XGBoost
predictions_glm <- exp(predict(glm_model, newdata = scaled_data_test, type = "response")) - 1
predictions_xgb <- exp(predict(xgb_model, dtest)) - 1
# Apply logistic transformation
probabilities_glm <- logistic(predictions_glm)
probabilities_xgb <- logistic(predictions_xgb)
# Output probabilities
print(probabilities_glm)
print(probabilities_xgb)
hist(probabilities_glm)
hist(probabilities_xgb)
library(caret)
calib_data <- data.frame(predicted = predictions_xgb, actual = info_tr_test$ClaimNb_cap / info_tr_test$Exposure)
# Fit Platt scaling model
platt_model <- train(predicted ~ actual, data = calib_data, method = "glm", family = "binomial")
View(info_tr_cal)
# Logistic transformation
logistic <- function(x) {
1 / (1 + exp(-x))
}
# Calculate predictions for GLM and XGBoost
predictions_glm <- exp(predict(glm_model, newdata = scaled_data_test, type = "response")) - 1
predictions_xgb <- exp(predict(xgb_model, dtest)) - 1
# Apply logistic transformation
probabilities_glm <- logistic(predictions_glm)
probabilities_xgb <- logistic(predictions_xgb)
# Output probabilities
print(probabilities_glm)
print(probabilities_xgb)
hist(probabilities_glm)
hist(probabilities_xgb)
scaled_data_cal <- cbind(info_tr_cal$ClaimNb_cap/info_tr_cal$Exposure,info_tr_cal)
colnames(scaled_data_cal) <- c("Scaled_ClaimNb",colnames(info_tr_cal))
scaled_data_cal <- scaled_data_cal[, -c(2,3,13)]
scaled_data_train <- cbind(info_tr_train$ClaimNb_cap/info_tr_train$Exposure,info_tr_train)
colnames(scaled_data_train) <- c("Scaled_ClaimNb",colnames(info_tr_train))
scaled_data_train <- scaled_data_train[, -c(2,3,13)]
scaled_data_test <- cbind(info_tr_test$ClaimNb_cap/info_tr_test$Exposure,info_tr_test)
colnames(scaled_data_test) <- c("Scaled_ClaimNb",colnames(info_tr_test))
scaled_data_test <- scaled_data_test[, -c(2,3,13)]
scaled_data_cal <- cbind(info_tr_cal$ClaimNb_cap/info_tr_cal$Exposure,info_tr_cal)
colnames(scaled_data_cal) <- c("Scaled_ClaimNb",colnames(info_tr_cal))
scaled_data_cal <- scaled_data_cal[, -c(2,3,13)]
# Train
# log-transformation
scaled_data_train$log_Scaled_ClaimNb <- log(scaled_data_train$Scaled_ClaimNb + 1)
#scaled_data_train <- #scaled_data_train[scaled_data_train$Scaled_ClaimNb < quantile(scaled_data_train$Scaled_ClaimNb, 0.99),]
# Categorical to factors
scaled_data_train$Area <- as.factor(scaled_data_train$Area)
scaled_data_train$VehBrand <- as.factor(scaled_data_train$VehBrand)
scaled_data_train$VehGas <- as.factor(scaled_data_train$VehGas)
scaled_data_train$Region <- as.factor(scaled_data_train$Region)
scaled_data_train <- scaled_data_train[, -c(1)]
# Test
# log-transformation
scaled_data_test$log_Scaled_ClaimNb <- log(scaled_data_test$Scaled_ClaimNb + 1)
#scaled_data_test <- #scaled_data_test[scaled_data_test$Scaled_ClaimNb < quantile(scaled_data_test$Scaled_ClaimNb, 0.99),]
# Categorical to factors
scaled_data_test$Area <- as.factor(scaled_data_test$Area)
scaled_data_test$VehBrand <- as.factor(scaled_data_test$VehBrand)
scaled_data_test$VehGas <- as.factor(scaled_data_test$VehGas)
scaled_data_test$Region <- as.factor(scaled_data_test$Region)
scaled_data_test <- scaled_data_test[, -c(1)]
# Cal
# log-transformation
scaled_data_cal$log_Scaled_ClaimNb <- log(scaled_data_cal$Scaled_ClaimNb + 1)
# Categorical to factors
scaled_data_cal$Area <- as.factor(scaled_data_cal$Area)
scaled_data_cal$VehBrand <- as.factor(scaled_data_cal$VehBrand)
scaled_data_cal$VehGas <- as.factor(scaled_data_cal$VehGas)
scaled_data_cal$Region <- as.factor(scaled_data_cal$Region)
scaled_data_cal <- scaled_data_cal[, -c(1)]
library(dplyr)
library(tidyr)
# Data Load ----
url <- "https://github.com/dutangc/CASdatasets/blob/master/data/freMTPL2freq.rda?raw=true"
load(url(url))
url <- "https://github.com/dutangc/CASdatasets/blob/master/data/freMTPL2sev.rda?raw=true"
load(url(url))
#########################################################################################################################
# Will have 2 sets of data: Original, Transformed features #
#########################################################################################################################
# For each set we will have 3 data objects:
# info - dataset to train the number of claims model
# comb - dataset to train the amount of a claim model
# comb_agg - dataset to test model for modelling the total amount for a policy
# Then to split the datasets we split the info and comb dataset
# Then in order to get the conformal intervals and check the coverage for the total claims we use the same split as used for the info set
# Using stratified sampling to generate the 3 sets
#### Original ####
freMTPL2freq$Exposure = pmin(freMTPL2freq$Exposure, 1)
info = freMTPL2freq ; claims = freMTPL2sev
# rm(freMTPL2freq);rm(freMTPL2sev)
# removing outliers
# for the info dataset truncated the number of claims to 4
info$ClaimNb_cap = pmin(info$ClaimNb, 4)
info = select(info, -"ClaimNb")
